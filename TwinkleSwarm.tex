\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}

\geometry{margin=1in}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\title{\texttt{TwinkleSwarm}\\
	\large Illuminated Drone Swarm Coordination}
\author{Stochastic Batman}
\date{January 11, 2026}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		This document provides a mathematical foundation for \texttt{TwinkleSwarm}, a simulation framework for coordinating illuminated drone swarms. We present the differential equations governing drone motion, analyze their behavior, and describe the algorithms for extracting target formations from images and motion patterns from videos. The framework combines control theory, particle dynamics, and computer vision to achieve collision-free formation control. \texttt{Claude Sonnet 4.5} was used to correct grammar, format math quickly, and check for errors.
	\end{abstract}
	
	\section{Introduction}
	
	\texttt{TwinkleSwarm} simulates coordinated drone swarms that form visual patterns extracted from images or follow motion extracted from videos. Each drone is modeled as a particle with position and velocity, subject to control forces that drive it toward target positions while avoiding collisions with neighboring drones.
	
	The system addresses three sequential challenges:
	\begin{enumerate}
		\item \textbf{Static Formation}: Moving drones from arbitrary initial positions to form a handwritten pattern
		\item \textbf{Shape Transition}: Smoothly transitioning the swarm from one formation to another
		\item \textbf{Dynamic Tracking}: Following moving objects in video while preserving the swarm's shape
	\end{enumerate}
	
	\section{Mathematical Model}
	
	\subsection{State Representation}
	
	Each drone $i$ (where $i = 1, 2, \ldots, N$) is characterized by:
	\begin{itemize}
		\item $\mathbf{x}_i(t) \in \mathbb{R}^3$: Position at time $t$
		\item $\mathbf{v}_i(t) \in \mathbb{R}^3$: Velocity at time $t$
	\end{itemize}
	
	The complete swarm state at time $t$ is the collection of all drone states: $(\mathbf{x}_1, \mathbf{v}_1, \mathbf{x}_2, \mathbf{v}_2, \ldots, \mathbf{x}_N, \mathbf{v}_N)$.
	
	\subsection{Governing Equations}
	
	The motion of each drone follows a second-order system with velocity saturation and inter-drone repulsion. We present the Initial Value Problem (IVP) formulation for time-forward simulation.
	
	\subsubsection{Initial Value Problem (IVP) Formulation}
	
	The IVP formulation describes how drones evolve from known initial conditions:
	
	\begin{equation}
		\dot{\mathbf{x}}_i(t) = \mathbf{v}_i(t) \cdot \min\left(1, \frac{v_{\max}}{\|\mathbf{v}_i(t)\|}\right)
		\label{eq:position_update}
	\end{equation}
	
	\begin{equation}
		\dot{\mathbf{v}}_i(t) = \frac{1}{m}\left[k_p(\mathbf{T}_i(t) - \mathbf{x}_i(t)) + \sum_{j \neq i} \mathbf{f}_{\text{rep}}(\mathbf{x}_i(t), \mathbf{x}_j(t)) - k_d \mathbf{v}_i(t)\right]
		\label{eq:velocity_update_ivp}
	\end{equation}
	
	with initial conditions:
	\begin{equation}
		\mathbf{x}_i(0) = \mathbf{x}_{i,0}, \quad \mathbf{v}_i(0) = \mathbf{v}_{i,0}
	\end{equation}
	
	\textbf{Intuition}: Equation \eqref{eq:position_update} updates position based on velocity, but caps the speed at $v_{\max}$ (a physical constraint). Equation \eqref{eq:velocity_update_ivp} describes how velocity changes over time, driven by three forces:
	\begin{itemize}
		\item \textbf{Attraction to target}: $k_p(\mathbf{T}_i - \mathbf{x}_i)$ pulls the drone toward its assigned target $\mathbf{T}_i$
		\item \textbf{Repulsion from neighbors}: $\sum_{j \neq i} \mathbf{f}_{\text{rep}}$ pushes drones apart to avoid collisions
		\item \textbf{Damping}: $-k_d \mathbf{v}_i$ reduces velocity over time, preventing oscillation and ensuring smooth convergence
	\end{itemize}
	
	\textbf{Parameters}:
	\begin{itemize}
		\item $m > 0$: Drone mass (constant)
		\item $k_p > 0$: Proportional gain controlling attraction strength
		\item $k_d > 0$: Damping coefficient controlling convergence speed
		\item $v_{\max} > 0$: Maximum allowable velocity
		\item $\mathbf{T}_i(t) \in \mathbb{R}^3$: Time-varying target position for drone $i$
	\end{itemize}
	
	\subsubsection{Repulsive Force Model}
	
	To prevent collisions, drones repel each other when too close. The repulsive force between drones $i$ and $j$ is:
	
	\begin{equation}
		\mathbf{f}_{\text{rep}}(\mathbf{x}_i, \mathbf{x}_j) = \begin{cases}
			k_{\text{rep}} \cdot \frac{\mathbf{x}_i - \mathbf{x}_j}{\|\mathbf{x}_i - \mathbf{x}_j\|^3} & \text{if } \|\mathbf{x}_i - \mathbf{x}_j\| < R_{\text{safe}} \\
			\mathbf{0} & \text{otherwise}
		\end{cases}
		\label{eq:repulsion}
	\end{equation}
	
	where $k_{\text{rep}} > 0$ is the repulsion gain and $R_{\text{safe}} > 0$ is the safety radius.
	
	\textbf{Intuition}: The force follows an inverse-cube law, meaning it grows rapidly as drones get closer. The direction $\frac{\mathbf{x}_i - \mathbf{x}_j}{\|\mathbf{x}_i - \mathbf{x}_j\|}$ is the unit vector pointing from drone $j$ to drone $i$, so the force pushes drone $i$ away from drone $j$. The cubic dependence ensures strong short-range repulsion while allowing free movement when drones are far apart.
	
	\subsection{Velocity Fields and Saturation}
	
	\subsubsection{What is a Velocity Field?}
	
	A velocity field is a function $\mathbf{V}: \mathbb{R}^3 \times \mathbb{R} \to \mathbb{R}^3$ that assigns a velocity vector to each point in space and time:
	
	\begin{equation}
		\mathbf{V}(\mathbf{x}, t) = \begin{bmatrix} V_x(\mathbf{x}, t) \\ V_y(\mathbf{x}, t) \\ V_z(\mathbf{x}, t) \end{bmatrix}
	\end{equation}
	
	\textbf{Intuition}: Think of a velocity field like a wind map. At every location $\mathbf{x}$ and time $t$, the field tells you "if you were at this position, here's how fast and in what direction you should be moving". For drone swarms following video motion, the velocity field describes how the target shape is moving through space.
	
	Unlike static targets $\mathbf{T}_i$ which tell drones "where to go", a velocity field tells them "how to move". For dynamic tracking: when following a moving object, we don't just want to know where the object is now, but how it's moving so we can move with it.
	
	\subsubsection{The Saturation Problem}
	
	In practice, velocity fields extracted from videos can contain arbitrarily large velocities (computer vision algorithms are not perfect, yet). A fast-moving object might have $\|\mathbf{V}(\mathbf{x}, t)\| \gg v_{\max}$, where $v_{\max}$ is the physical velocity limit of our drones.
	
	\textbf{Why We Care About Saturation}:
	\begin{itemize}
		\item \textbf{Physical constraints}: Real drones have maximum speeds determined by motor capabilities, battery power, and aerodynamics.
		\item \textbf{Safety}: Exceeding velocity limits can lead to loss of control or collisions.
		\item \textbf{Simulation validity}: If we allow unbounded velocities in simulation, our results won't transfer to real hardware.
		\item \textbf{Numerical stability}: Very large velocities can cause integration schemes to become unstable or require impractically small time steps.
	\end{itemize}
	
	\textbf{The Naive Approach Fails}: Simply using $\dot{\mathbf{v}}_i = \mathbf{V}(\mathbf{x}_i, t) - \mathbf{v}_i$ doesn't respect velocity limits. Even if we clip drone velocities after integration, the control law itself doesn't "know" about the constraint, leading to poor tracking and potential instability.
	
	\subsubsection{Saturated Velocity Field}
	
	To address this, we define the saturated velocity field:
	
	\begin{equation}
		\mathbf{V}_{\text{sat}}(\mathbf{x}, t) = \begin{cases}
			\mathbf{V}(\mathbf{x}, t) \cdot \min\left(1, \frac{v_{\max}}{\|\mathbf{V}(\mathbf{x}, t)\|}\right) & \text{if } \|\mathbf{V}(\mathbf{x}, t)\| > 0 \\
			\mathbf{0} & \text{otherwise}
		\end{cases}
		\label{eq:velocity_saturation}
	\end{equation}
	
	\textbf{How It Works}: 
	\begin{itemize}
		\item If $\|\mathbf{V}(\mathbf{x}, t)\| \leq v_{\max}$, then $\min(1, \frac{v_{\max}}{\|\mathbf{V}\|}) = 1$ and $\mathbf{V}_{\text{sat}} = \mathbf{V}$ (no change)
		\item If $\|\mathbf{V}(\mathbf{x}, t)\| > v_{\max}$, then $\min(1, \frac{v_{\max}}{\|\mathbf{V}\|}) = \frac{v_{\max}}{\|\mathbf{V}\|} < 1$, which scales the vector down
	\end{itemize}
	
	The result is that $\mathbf{V}_{\text{sat}}$ always satisfies $\|\mathbf{V}_{\text{sat}}(\mathbf{x}, t)\| \leq v_{\max}$.
	
	\textbf{Geometric Interpretation}: Saturation preserves direction but limits magnitude. If you visualize the velocity field as a collection of arrows, saturation "clips" the long arrows to maximum length while leaving short arrows unchanged. The field retains its qualitative structure (which direction motion is flowing) while respecting physical constraints.
	
	\textbf{Example}: Suppose $v_{\max} = 5$ m/s and at some point $\mathbf{V} = (12, 9, 0)^T$ m/s. Then $\|\mathbf{V}\| = 15$ m/s. The saturated version is:
	\begin{equation*}
		\mathbf{V}_{\text{sat}} = (12, 9, 0)^T \cdot \frac{5}{15} = (4, 3, 0)^T \text{ m/s}
	\end{equation*}
	The direction (north-east) is preserved, but the speed is reduced from 15 to 5 m/s.
	
	\subsection{Velocity Field Tracking (for Dynamic Scenarios)}
	
	When tracking motion from video, we use a velocity field formulation instead of fixed targets:
	
	\begin{equation}
		\dot{\mathbf{x}}_i(t) = \mathbf{v}_i(t) \cdot \min\left(1, \frac{v_{\max}}{\|\mathbf{v}_i(t)\|}\right)
	\end{equation}
	
	\begin{equation}
		\dot{\mathbf{v}}_i(t) = \frac{1}{m}\left[k_v(\mathbf{V}_{\text{sat}}(\mathbf{x}_i(t), t) - \mathbf{v}_i(t)) + \sum_{j \neq i} \mathbf{f}_{\text{rep}}(\mathbf{x}_i(t), \mathbf{x}_j(t)) - k_d \mathbf{v}_i(t)\right]
		\label{eq:velocity_field_tracking}
	\end{equation}
	
	where $\mathbf{V}(\mathbf{x}, t)$ is a velocity field extracted from video (see Section \ref{sec:video_processing}), and $\mathbf{V}_{\text{sat}}$ is its saturated version.
	
	\textbf{Intuition}: Instead of moving toward fixed target positions, drones now try to match their velocities to a field $\mathbf{V}(\mathbf{x}, t)$ that describes how the target shape is moving. The term $k_v(\mathbf{V}_{\text{sat}} - \mathbf{v}_i)$ drives each drone's velocity toward the field velocity at its current position. This allows the swarm to track dynamic motion while maintaining its formation.
	
	\subsection{Analytical Solutions}
	
	\subsubsection{Single Drone, No Repulsion}
	
	Consider a single drone ($N=1$) with no repulsive forces, moving toward a fixed target $\mathbf{T}$ (independent of time):
	
	\begin{align}
		\dot{\mathbf{x}}(t) &= \mathbf{v}(t) \\
		\dot{\mathbf{v}}(t) &= \frac{1}{m}[k_p(\mathbf{T} - \mathbf{x}(t)) - k_d \mathbf{v}(t)]
	\end{align}
	
	This is a linear system. Define the error $\mathbf{e}(t) = \mathbf{x}(t) - \mathbf{T}$. Then:
	
	\begin{align}
		\dot{\mathbf{e}}(t) &= \mathbf{v}(t) \\
		\dot{\mathbf{v}}(t) &= -\frac{k_p}{m}\mathbf{e}(t) - \frac{k_d}{m}\mathbf{v}(t)
	\end{align}
	
	To demonstrate that this system represents a damped harmonic oscillator, we can reduce the coupled first-order equations into a single second-order differential equation.
	
	Differentiating the first equation with respect to time gives the acceleration:
	\begin{equation}
		\ddot{\mathbf{e}}(t) = \dot{\mathbf{v}}(t)
	\end{equation}
	
	Substituting the expression for from the second equation yields:
	\begin{equation}
		\ddot{\mathbf{e}}(t) = -\frac{k_p}{m}\mathbf{e}(t) - \frac{k_d}{m}\mathbf{v}(t) = -\frac{k_p}{m}\mathbf{e}(t) - \frac{k_d}{m}\dot{\mathbf{e}}(t)
	\end{equation}
	
	Rearranging results in the standard second-order linear homogeneous ODE:
	\begin{equation}
		\ddot{\mathbf{e}}(t) + \frac{k_d}{m}\dot{\mathbf{e}}(t) + \frac{k_p}{m}\mathbf{e}(t) = \mathbf{0}
	\end{equation}
	
	This is a damped harmonic oscillator. Assume a solution of the form $e(t) = e^{\lambda t}$, then as $e^{\lambda t} \neq 0$, divide by $e^{\lambda t}$. Then, the characteristic equation is: \begin{equation} \lambda^2 + \frac{k_d}{m}\lambda + \frac{k_p}{m} = 0 \end{equation}
	with roots:
	\begin{equation}
		\lambda_{1,2} = \frac{-k_d \pm \sqrt{k_d^2 - 4mk_p}}{2m}
	\end{equation}
	
	\begin{enumerate}
		\item[Case 1:] \textbf{Overdamped} ($k_d^2 > 4mk_p$). Both roots are real and negative, leading to exponential decay without oscillation.
		\item[Case 2:] \textbf{Critically Damped} ($k_d^2 = 4mk_p$). A repeated negative root, yielding the fastest convergence without overshoot.
		\item[Case 3:] \textbf{Underdamped} ($k_d^2 < 4mk_p$). Complex conjugate roots with negative real part, resulting in damped oscillations around the target.
		\item[Stability:] Because $k_d$ and $m$ are positive, the real part of $\lambda$ is always negative, which ensures that $\mathbf{e}(t) \to \mathbf{0}$ as $t \to \infty$ and the drones actually reach their targets.
	\end{enumerate}
	
	The general solution for the underdamped case is:
	
	\begin{equation}
		\mathbf{e}(t) = e^{-\alpha t}(\mathbf{A}\cos(\omega t) + \mathbf{B}\sin(\omega t))
	\end{equation}
	
	where $\alpha = \frac{k_d}{2m}$ (decay rate) and $\omega = \frac{\sqrt{4mk_p - k_d^2}}{2m}$ (oscillation frequency). The coefficients $\mathbf{A}$ and $\mathbf{B}$ depend on initial conditions.
	
	\subsubsection{Energy Analysis}
	
	Define the total energy of the system as:
	
	\begin{equation}
		E(t) = \sum_{i=1}^{N} \left[\frac{1}{2}m\|\mathbf{v}_i(t)\|^2 + \frac{1}{2}k_p\|\mathbf{x}_i(t) - \mathbf{T}_i(t)\|^2\right]
	\end{equation}
	
	The first term is kinetic energy, the second is potential energy from the attraction to targets. Taking the time derivative (ignoring repulsion and time-varying targets for simplicity):
	
	\begin{equation}
		\frac{dE}{dt} = \sum_{i=1}^{N} \left[m\mathbf{v}_i \cdot \dot{\mathbf{v}}_i + k_p(\mathbf{x}_i - \mathbf{T}_i) \cdot \dot{\mathbf{x}}_i\right]
	\end{equation}
	
	Substituting the dynamics:
	
	\begin{align}
		\frac{dE}{dt} &= \sum_{i=1}^{N} \left[\mathbf{v}_i \cdot (k_p(\mathbf{T}_i - \mathbf{x}_i) - k_d\mathbf{v}_i) + k_p(\mathbf{x}_i - \mathbf{T}_i) \cdot \mathbf{v}_i\right] \\
		&= \sum_{i=1}^{N} \left[-k_d\|\mathbf{v}_i\|^2\right] \\
		&= -k_d \sum_{i=1}^{N} \|\mathbf{v}_i\|^2 \leq 0
	\end{align}
	
	\textbf{Interpretation}: Energy decreases over time due to damping, ensuring the system converges to a steady state where all drones are stationary at their targets.
	
	\section{Video Processing and Motion Extraction}
	\label{sec:video_processing}
	
	This section describes how we extract motion information from video to drive the drone swarm. The key idea is to compute a \textit{velocity field} that describes how pixels move between consecutive frames, then use this field to guide drone motion.
	
	\subsection{The Motion Extraction Problem}
	
	Given a video sequence $\{I_1, I_2, \ldots, I_T\}$ where each $I_t: \Omega \to [0, 1]$ is a grayscale image defined on domain $\Omega \subset \mathbb{R}^2$, we want to find a velocity field $\mathbf{V}: \Omega \times \mathbb{R} \to \mathbb{R}^2$ that describes how the image content moves over time.
	
	\textbf{Intuition}: Imagine watching a ball roll across the screen. At each moment, different pixels correspond to the ball's surface. As the ball moves, these pixels appear to "flow" across the image. The velocity field captures this flow: at each pixel location and time, it tells us which direction and how fast the visual content is moving.
	
	\subsection{Optical Flow}
	
	Optical flow is the pattern of apparent motion of objects in a visual scene caused by relative motion between the observer and the scene. We compute optical flow between consecutive frames to extract this motion.
	
	\subsubsection{The Brightness Constancy Assumption}
	
	The fundamental assumption underlying most optical flow methods is \textbf{brightness constancy}: a point in the scene maintains constant brightness as it moves between frames. Mathematically, if a pixel at location $(x, y)$ in frame $t$ moves to location $(x + u, y + v)$ in frame $t+1$, then:
	
	\begin{equation}
		I_t(x, y) = I_{t+1}(x + u, y + v)
	\end{equation}
	
	where $(u, v)$ is the displacement vector (the flow).
	
	\textbf{Why This Makes Sense}: If you're tracking a red dot moving across a white background, the dot doesn't change color as it moves - only its position changes. The brightness constancy assumption formalizes this: the intensity value "travels" with the motion.
	
	\textbf{When It Fails}: This assumption breaks down when:
	\begin{itemize}
		\item Lighting changes (shadows appear/disappear)
		\item Objects rotate (different surfaces become visible)
		\item Occlusions occur (objects pass behind each other)
		\item Motion blur is significant
	\end{itemize}
	
	Despite these limitations, brightness constancy works well for small displacements and consistent lighting.
	
	\subsubsection{Deriving the Optical Flow Equation}
	
	To convert the brightness constancy assumption into a usable equation, we perform a first-order Taylor expansion of $I_{t+1}(x + u, y + v)$ around $(x, y)$:
	
	\begin{equation}
		I_{t+1}(x + u, y + v) \approx I_{t+1}(x, y) + \frac{\partial I_{t+1}}{\partial x}u + \frac{\partial I_{t+1}}{\partial y}v
	\end{equation}
	
	This is valid when displacements $(u, v)$ are small. Now, assuming the image changes smoothly over time, we can approximate:
	
	\begin{equation}
		I_{t+1}(x, y) \approx I_t(x, y) + \frac{\partial I_t}{\partial t} \cdot \Delta t
	\end{equation}
	
	where $\Delta t$ is the time between frames (typically $\Delta t = 1$ frame). For small time intervals, $\frac{\partial I_{t+1}}{\partial x} \approx \frac{\partial I_t}{\partial x}$ and similarly for $y$. Substituting the Taylor expansion into the brightness constancy equation:
	
	\begin{equation}
		I_t(x, y) = I_t(x, y) + \frac{\partial I_t}{\partial t} + \frac{\partial I_t}{\partial x}u + \frac{\partial I_t}{\partial y}v
	\end{equation}
	
	Canceling $I_t(x, y)$ from both sides:
	
	\begin{equation}
		\frac{\partial I_t}{\partial x}u + \frac{\partial I_t}{\partial y}v + \frac{\partial I_t}{\partial t} = 0
		\label{eq:optical_flow_constraint}
	\end{equation}
	
	This is the \textbf{optical flow constraint equation}. In compact notation:
	
	\begin{equation}
		\nabla I_t \cdot \mathbf{v}_f + I_t' = 0
	\end{equation}
	
	where:
	\begin{itemize}
		\item $\nabla I_t = \left(\frac{\partial I_t}{\partial x}, \frac{\partial I_t}{\partial y}\right)^T$ is the spatial gradient (direction of brightness change)
		\item $\mathbf{v}_f = (u, v)^T$ is the velocity (flow) vector
		\item $I_t' = \frac{\partial I_t}{\partial t}$ is the temporal derivative (brightness change over time)
	\end{itemize}
	
	\textbf{Geometric Interpretation}: Equation \eqref{eq:optical_flow_constraint} says that if brightness is constant along motion, then the rate of brightness change over time ($I_t'$) must be balanced by motion along the brightness gradient ($\nabla I_t \cdot \mathbf{v}_f$). If a pixel moves toward brighter regions at the right speed, the temporal darkening cancels the spatial brightening.
	
	\textbf{The Aperture Problem}: This is one equation with two unknowns ($u$ and $v$). Geometrically, we can only determine the component of flow perpendicular to image edges - motion parallel to edges is invisible. This is called the \textit{aperture problem}. To solve it, we need additional constraints.
	
	\subsection{Lucas-Kanade Method}
	
	The Lucas-Kanade method resolves the aperture problem by assuming the flow is \textbf{constant in a local neighborhood} $\mathcal{N}$ around each pixel. This is reasonable for small patches where all pixels belong to the same moving object.
	
	\subsubsection{Formulation}
	
	For a neighborhood $\mathcal{N}$ centered at pixel $(x_0, y_0)$, we assume $(u, v)$ is constant across all pixels $(x, y) \in \mathcal{N}$. The optical flow constraint should hold at every pixel, but due to noise and modeling errors, we solve it in a least-squares sense:
	
	\begin{equation}
		\min_{u, v} \sum_{(x, y) \in \mathcal{N}} \left[\frac{\partial I_t}{\partial x}(x, y) \cdot u + \frac{\partial I_t}{\partial y}(x, y) \cdot v + \frac{\partial I_t}{\partial t}(x, y)\right]^2
	\end{equation}
	
	\textbf{Intuition}: We're fitting a single velocity $(u, v)$ that best explains the brightness changes across all pixels in the patch. Some pixels might not fit perfectly (due to noise), but the least-squares solution minimizes overall error.
	
	\subsubsection{Matrix Form}
	
	Let $I_x = \frac{\partial I_t}{\partial x}$, $I_y = \frac{\partial I_t}{\partial y}$, and $I_t = \frac{\partial I_t}{\partial t}$. The least-squares problem can be written in matrix form. Define:
	
	\begin{equation}
		A = \begin{bmatrix}
			I_x(x_1, y_1) & I_y(x_1, y_1) \\
			I_x(x_2, y_2) & I_y(x_2, y_2) \\
			\vdots & \vdots \\
			I_x(x_n, y_n) & I_y(x_n, y_n)
		\end{bmatrix}, \quad
		\mathbf{b} = -\begin{bmatrix}
			I_t(x_1, y_1) \\
			I_t(x_2, y_2) \\
			\vdots \\
			I_t(x_n, y_n)
		\end{bmatrix}, \quad
		\mathbf{v} = \begin{bmatrix} u \\ v \end{bmatrix}
	\end{equation}
	
	where $(x_i, y_i)$ are the $n$ pixels in $\mathcal{N}$. The least-squares solution minimizes $\|A\mathbf{v} - \mathbf{b}\|^2$, which is:
	
	\begin{equation}
		\mathbf{v} = (A^T A)^{-1} A^T \mathbf{b}
	\end{equation}
	
	Computing $A^T A$ and $A^T \mathbf{b}$:
	
	\begin{equation}
		A^T A = \begin{bmatrix}
			\sum I_x^2 & \sum I_x I_y \\
			\sum I_x I_y & \sum I_y^2
		\end{bmatrix}, \quad
		A^T \mathbf{b} = -\begin{bmatrix}
			\sum I_x I_t \\
			\sum I_y I_t
		\end{bmatrix}
	\end{equation}
	
	where all sums are over $(x, y) \in \mathcal{N}$. Therefore:
	
	\begin{equation}
		\begin{bmatrix} u \\ v \end{bmatrix} = -\begin{bmatrix}
			\sum I_x^2 & \sum I_x I_y \\
			\sum I_x I_y & \sum I_y^2
		\end{bmatrix}^{-1} \begin{bmatrix}
			\sum I_x I_t \\
			\sum I_y I_t
		\end{bmatrix}
		\label{eq:lucas_kanade_solution}
	\end{equation}
	
	\subsubsection{Interpreting the Structure Tensor}
	
	The matrix $A^T A$ is called the \textbf{structure tensor} or \textbf{second moment matrix}. Its properties determine whether the flow can be reliably computed:
	
	\begin{itemize}
		\item \textbf{Large eigenvalues in both directions}: The patch contains texture in multiple orientations (e.g., a corner). Flow is well-determined.
		\item \textbf{One large eigenvalue}: The patch contains an edge. Flow perpendicular to the edge is determined, but parallel flow is ambiguous (aperture problem).
		\item \textbf{Small eigenvalues}: The patch is uniform (constant intensity). No reliable flow can be computed.
	\end{itemize}
	
	In practice, we only compute flow at pixels where $A^T A$ is well-conditioned, typically by checking that its smallest eigenvalue exceeds a threshold.
	
	\subsection{Farneback Method}
	
	The Farneback method is a dense optical flow algorithm that approximates the neighborhood of each pixel with polynomial expansions. Unlike Lucas-Kanade which assumes constant flow in a local neighborhood, Farneback models the image intensity as a second-order polynomial and estimates displacement by comparing polynomial coefficients between frames.
	
	\subsubsection{Polynomial Expansion Model}
	
	The fundamental idea is to represent the image intensity $I(\mathbf{x})$ in a neighborhood around each point using a quadratic polynomial:
	
	\begin{equation}
		I(\mathbf{x}) \approx \mathbf{x}^T \mathbf{A} \mathbf{x} + \mathbf{b}^T \mathbf{x} + c
		\label{eq:polynomial_expansion}
	\end{equation}
	
	where $\mathbf{A}$ is a symmetric $2 \times 2$ matrix, $\mathbf{b}$ is a $2 \times 1$ vector, and $c$ is a scalar constant. This expansion is computed using a weighted least-squares fit over the neighborhood, typically with a Gaussian weighting function.
	
	\subsubsection{Displacement Estimation}
	
	Consider two consecutive frames $I_1$ and $I_2$. Under the assumption of brightness constancy and small displacement $\mathbf{d} = (u, v)^T$, we have:
	
	\begin{equation}
		I_2(\mathbf{x}) = I_1(\mathbf{x} - \mathbf{d})
		\label{eq:brightness_constancy_farneback}
	\end{equation}
	
	If we expand $I_1$ around $\mathbf{x}$ using the polynomial model:
	\begin{align}
		I_1(\mathbf{x} - \mathbf{d}) &\approx (\mathbf{x} - \mathbf{d})^T \mathbf{A}_1 (\mathbf{x} - \mathbf{d}) + \mathbf{b}_1^T (\mathbf{x} - \mathbf{d}) + c_1 \\
		&= \mathbf{x}^T \mathbf{A}_1 \mathbf{x} + (\mathbf{b}_1 - 2\mathbf{A}_1\mathbf{d})^T \mathbf{x} + (c_1 - \mathbf{b}_1^T\mathbf{d} + \mathbf{d}^T\mathbf{A}_1\mathbf{d})
	\end{align}
	
	Comparing this with the polynomial expansion of $I_2$ at point $\mathbf{x}$:
	\begin{equation}
		I_2(\mathbf{x}) = \mathbf{x}^T \mathbf{A}_2 \mathbf{x} + \mathbf{b}_2^T \mathbf{x} + c_2
	\end{equation}
	
	Equating coefficients gives the system:
	\begin{align}
		\mathbf{A}_2 &= \mathbf{A}_1 \\
		\mathbf{b}_2 &= \mathbf{b}_1 - 2\mathbf{A}_1\mathbf{d} \\
		c_2 &= c_1 - \mathbf{b}_1^T\mathbf{d} + \mathbf{d}^T\mathbf{A}_1\mathbf{d}
	\end{align}
	
	From the second equation, we can solve for the displacement:
	\begin{equation}
		\mathbf{d} = -\frac{1}{2}\mathbf{A}_1^{-1}(\mathbf{b}_2 - \mathbf{b}_1)
		\label{eq:farneback_displacement}
	\end{equation}
	
	provided $\mathbf{A}_1$ is invertible (i.e., the neighborhood has sufficient texture).
	
	\subsubsection{Practical Implementation}
	
	In practice, the Farneback algorithm as implemented in OpenCV's \texttt{cv2.calcOpticalFlowFarneback()}.

	
	\textbf{Advantages over Lucas-Kanade}:
	\begin{itemize}
		\item Produces dense flow fields (flow at every pixel)
		\item More robust to larger displacements
		\item Better handling of textureless regions through global constraints
	\end{itemize}
	
	\textbf{Disadvantages}:
	\begin{itemize}
		\item Computationally more expensive than sparse methods
		\item Requires careful parameter tuning
		\item May oversmooth small motions
	\end{itemize}
	
	For \texttt{TwinkleSwarm}, we use the Farneback method as implemented in OpenCV because it provides dense flow fields that are essential for tracking entire shapes in video frames.
	
	\subsection{Computing Spatial and Temporal Derivatives}
	
	To apply the Lucas-Kanade method, we need to compute $I_x$, $I_y$, and $I_t$ numerically from discrete images.
	
	\subsubsection{Spatial Gradients}
	
	The spatial derivatives are approximated using finite differences. Common choices include:
	
	\textbf{Central difference} (second-order accurate):
	\begin{equation}
		I_x(x, y) \approx \frac{I_t(x+1, y) - I_t(x-1, y)}{2}
	\end{equation}
	
	\textbf{Sobel operator} (more robust to noise):
	\begin{equation}
		I_x = \begin{bmatrix}
			-1 & 0 & 1 \\
			-2 & 0 & 2 \\
			-1 & 0 & 1
		\end{bmatrix} * I_t
	\end{equation}
	
	where $*$ denotes convolution. The Sobel operator applies smoothing (averaging in the $y$-direction) while differentiating in $x$, reducing sensitivity to noise.
	
	\subsubsection{Temporal Derivative}
	
	The temporal derivative is computed as:
	\begin{equation}
		I_t(x, y) \approx I_{t+1}(x, y) - I_t(x, y)
	\end{equation}
	
	For better accuracy, some implementations average over neighboring pixels:
	\begin{equation}
		I_t(x, y) \approx \frac{1}{9} \sum_{\Delta x \in \{-1, 0, 1\}} \sum_{\Delta y \in \{-1, 0, 1\}} \left[ I_{t+1}(x + \Delta x, y + \Delta y) - I_t(x + \Delta x, y + \Delta y) \right]
	\end{equation}
	
	\subsection{Velocity Field Construction for Drone Control}
	
	The optical flow computation gives us a 2D velocity field $\mathbf{V}_{\text{raw}}(x, y, t) = (u(x, y, t), v(x, y, t))^T$ in pixel coordinates. To use this for controlling drones in physical 3D space, we perform several transformations:
	
	\subsubsection*{Step 1: Filtering and Smoothing}
	
	Raw optical flow often contains noise and outliers. We apply spatial smoothing (e.g., Gaussian blur) to obtain a smoother field:
	\begin{equation}
		\mathbf{V}_{\text{smooth}} = G_\sigma * \mathbf{V}_{\text{raw}}
	\end{equation}
	
	where $G_\sigma$ is a Gaussian kernel with standard deviation $\sigma$.
	
	\subsubsection*{Step 2: Coordinate Transformation}
	
	Image coordinates have origin at the top-left corner with $y$ increasing downward, while our drone coordinate system has origin at the center with $y$ increasing upward. We transform:
	\begin{equation}
		\begin{bmatrix} x_{\text{drone}} \\ y_{\text{drone}} \end{bmatrix} = \begin{bmatrix}
			x_{\text{pixel}} - \frac{w}{2} \\
			\frac{h}{2} - y_{\text{pixel}}
		\end{bmatrix}
	\end{equation}
	
	where $w \times h$ is the image resolution. The velocity field transforms accordingly:
	\begin{equation}
		\mathbf{V}_{\text{drone}}(x, y, t) = \begin{bmatrix} u(x, y, t) \\ -v(x, y, t) \end{bmatrix}
	\end{equation}
	
	\subsubsection*{Step 3: Physical Scaling}
	
	Pixel velocities (pixels/frame) must be converted to physical velocities (meters/second). Let $s$ be the scale factor and $f$ the frame rate:
	\begin{equation}
		\mathbf{V}_{\text{physical}} = s \cdot f \cdot \mathbf{V}_{\text{drone}}
	\end{equation}
	
	For example, if $s = 0.01$ m/pixel and $f = 30$ fps, a flow of $(10, 5)$ pixels/frame becomes $(3.0, 1.5)$ m/s.
	
	\subsubsection*{Step 4: 3D Extension}
	
	Since videos provide only 2D motion, we extend to 3D by setting the $z$-component to zero:
	\begin{equation}
		\mathbf{V}(x, y, z, t) = \begin{bmatrix} V_x(x, y, t) \\ V_y(x, y, t) \\ 0 \end{bmatrix}
	\end{equation}
	
	This keeps the swarm motion in the plane parallel to the image.
	
	\subsubsection*{Step 5: Spatial Interpolation}
	
	Drones can be at arbitrary positions, not just pixel centers. We use bilinear interpolation to evaluate the velocity field at any point $(x, y)$:
	
	Let $(x, y)$ fall in the grid cell with corners at $(x_0, y_0)$, $(x_1, y_0)$, $(x_0, y_1)$, $(x_1, y_1)$. Define normalized coordinates:
	\begin{equation}
		\alpha = \frac{x - x_0}{x_1 - x_0}, \quad \beta = \frac{y - y_0}{y_1 - y_0}
	\end{equation}
	
	Then:
	\begin{align}
		\mathbf{V}(x, y, t) = &(1-\alpha)(1-\beta)\mathbf{V}(x_0, y_0, t) + \alpha(1-\beta)\mathbf{V}(x_1, y_0, t) \nonumber \\
		&+ (1-\alpha)\beta\mathbf{V}(x_0, y_1, t) + \alpha\beta\mathbf{V}(x_1, y_1, t)
	\end{align}
	
	\subsubsection*{Step 6: Saturation}
	
	Finally, we apply velocity saturation (as described in Section 2.3.3) to ensure $\|\mathbf{V}\| \leq v_{\max}$:
	\begin{equation}
		\mathbf{V}_{\text{sat}}(\mathbf{x}, t) = \begin{cases}
			\mathbf{V}(\mathbf{x}, t) \cdot \min\left(1, \frac{v_{\max}}{\|\mathbf{V}(\mathbf{x}, t)\|}\right) & \text{if } \|\mathbf{V}(\mathbf{x}, t)\| > 0 \\
			\mathbf{0} & \text{otherwise}
		\end{cases}
	\end{equation}
	
	\subsection{Practical Considerations}
	
	\subsubsection{Multi-Scale Optical Flow}
	
	For large displacements (when objects move many pixels between frames), the Taylor expansion underlying equation \eqref{eq:optical_flow_constraint} breaks down. The solution is to compute flow at multiple scales:
	
	\begin{enumerate}
		\item Create an image pyramid: $\{I^{(0)}, I^{(1)}, \ldots, I^{(L)}\}$ where $I^{(l+1)}$ is a downsampled version of $I^{(l)}$
		\item Compute flow at level $L$ (where motion is small)
		\item Upsample the flow and use it to warp $I^{(l-1)}$
		\item Compute residual flow at level $l-1$
		\item Repeat until reaching the original resolution
	\end{enumerate}
	
	This is called the \textbf{coarse-to-fine} or \textbf{pyramidal} approach.
	
	\subsubsection{Dense vs. Sparse Flow}
	
	\begin{itemize}
		\item \textbf{Dense flow}: Computed at every pixel. Good for tracking entire shapes but computationally expensive.
		\item \textbf{Sparse flow}: Computed only at feature points (corners, edges). Faster but may miss motion in textureless regions.
	\end{itemize}
	
	For drone swarms, dense flow is preferable because we want to track the entire visible shape, not just isolated points.
	
	\subsubsection{Alternative Methods}
	
	While we focus on Lucas-Kanade, other optical flow methods include:
	\begin{itemize}
		\item \textbf{Horn-Schunck}: Adds a global smoothness constraint, encouraging similar flow in neighboring pixels
		\item \textbf{Deep learning methods} (FlowNet, PWC-Net): Neural networks trained on large datasets
	\end{itemize}
	
	OpenCV provides implementations of these methods through \texttt{cv2.calcOpticalFlowFarneback()} and related functions.
	
	\section{Assignment Algorithms and Spatial Data Structures}
	
	\subsection{Hungarian (Munkres) Algorithm}
	
	The Hungarian algorithm, also known as the Munkres assignment algorithm, solves the assignment problem in polynomial time. Given $N$ drones and $N$ target positions, we need to assign each drone to a unique target to minimize the total squared distance traveled.
	
	\subsubsection{Problem Formulation}
	
	Let $C \in \mathbb{R}^{N \times N}$ be a cost matrix where $C_{ij}$ represents the cost of assigning drone $i$ to target $j$. In our case:
	\begin{equation}
		C_{ij} = \|\mathbf{x}_i(0) - \mathbf{T}_j\|^2
	\end{equation}
	
	where $\mathbf{x}_i(0)$ is the initial position of drone $i$ and $\mathbf{T}_j$ is the position of target $j$.
	
	We seek a permutation $\pi: \{1,\ldots,N\} \to \{1,\ldots,N\}$ that minimizes:
	\begin{equation}
		\sum_{i=1}^N C_{i,\pi(i)}
	\end{equation}
	
	\subsubsection{Algorithm Steps}
	
	The Hungarian algorithm operates on the cost matrix through a series of row and column operations:
	
	\begin{enumerate}
		\item \textbf{Row reduction}: For each row $i$, subtract the minimum element from all elements in that row:
		\begin{equation}
			C_{ij} \leftarrow C_{ij} - \min_k C_{ik} \quad \forall j
		\end{equation}
		
		\item \textbf{Column reduction}: For each column $j$, subtract the minimum element from all elements in that column:
		\begin{equation}
			C_{ij} \leftarrow C_{ij} - \min_k C_{kj} \quad \forall i
		\end{equation}
		
		\item \textbf{Cover zeros with minimum lines}: Find the minimum number of horizontal and vertical lines needed to cover all zeros in the matrix. If $N$ lines are needed (where $N$ is the matrix size), an optimal assignment exists among the zeros. Otherwise, proceed to step 4.
		
		\item \textbf{Create additional zeros}: Let $\delta$ be the smallest uncovered element. Subtract $\delta$ from all uncovered elements and add $\delta$ to all elements covered twice. Return to step 3.
	\end{enumerate}
	
	\subsubsection{Complexity and Implementation}
	
	The Hungarian algorithm has time complexity $O(N^3)$, which is efficient for moderate swarm sizes (up to several hundred drones). We use the implementation from SciPy's \texttt{linear\_sum\_assignment} function, which is based on the Jonker-Volgenant algorithm, a more efficient variant of the Hungarian algorithm.
	
	\textbf{Intuition}: The algorithm systematically reduces the cost matrix while maintaining the property that the optimal assignment remains unchanged. The row and column operations preserve the relative ordering of assignment costs, allowing us to find zero-cost assignments that correspond to optimal matchings.
	
	\subsection{K-d Trees for Spatial Queries}
	
	A $k$-d tree (short for $k$-dimensional tree) is a space-partitioning data structure for organizing points in $k$-dimensional space. For drone swarms in 3D space ($k=3$), $k$-d trees enable efficient neighbor searches essential for computing repulsive forces.
	
	\subsubsection{Data Structure}
	
	A $k$-d tree is a binary tree where each node represents a point in $\mathbb{R}^k$. Each non-leaf node splits the space along one of the coordinate axes using a hyperplane perpendicular to that axis. The tree is constructed recursively:
	
	\begin{enumerate}
		\item Choose the dimension with the greatest spread of points
		\item Select the median point along that dimension
		\item Create a node with this point
		\item Recursively build left subtree with points having coordinate less than median
		\item Recursively build right subtree with points having coordinate greater than median
	\end{enumerate}
	
	\subsubsection{Range Search Algorithm}
	
	To find all points within distance $R_{\text{safe}}$ of a query point $\mathbf{q}$, we perform a recursive search. The algorithm can be described as follows:
	
	\begin{algorithm}[H]
		\caption{K-d Tree Range Search}
		\begin{algorithmic}[1]
			\Procedure{RangeSearch}{node, $\mathbf{q}$, $R_{\text{safe}}$}
			\If{node is null}
			\State \Return
			\EndIf
			
			\State $d \gets \|\text{node.point} - \mathbf{q}\|$
			\If{$d < R_{\text{safe}}$}
			\State Add node.point to results
			\EndIf
			
			\State $\text{split\_dim} \gets \text{node.split\_dimension}$
			\State $\text{split\_val} \gets \text{node.point}[\text{split\_dim}]$
			\State $\text{q\_val} \gets \mathbf{q}[\text{split\_dim}]$
			
			\If{$\text{q\_val} - R_{\text{safe}} < \text{split\_val}$}
			\State \Call{RangeSearch}{node.left, $\mathbf{q}$, $R_{\text{safe}}$}
			\EndIf
			
			\If{$\text{q\_val} + R_{\text{safe}} > \text{split\_val}$}
			\State \Call{RangeSearch}{node.right, $\mathbf{q}$, $R_{\text{safe}}$}
			\EndIf
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	
	\subsubsection{Complexity Analysis}
	
	\begin{itemize}
		\item \textbf{Construction}: $O(N \log N)$ on average, $O(N^2)$ worst-case for poorly distributed points
		\item \textbf{Query}: $O(\log N)$ for nearest neighbor, $O(N^{1-1/k} + m)$ for range queries returning $m$ points
		\item \textbf{Memory}: $O(N)$ to store the tree structure
	\end{itemize}
	
	\subsubsection{Application to Drone Repulsion}
	
	For $N$ drones, naive pairwise distance checking would require $O(N^2)$ operations. Using $k$-d trees with range search reduces this to approximately $O(N \log N)$ for well-distributed drones. In \texttt{TwinkleSwarm}, we use SciPy's \texttt{KDTree} implementation which provides efficient methods like \texttt{query\_pairs} to find all pairs within a specified distance.
	
	\textbf{Hybrid Approach}: For small swarms ($N < 50$), we use brute-force (but using \texttt{numba}'s JIT) pairwise checking which has lower constant overhead. For larger swarms, we switch to $k$-d tree based neighbor search. This hybrid approach optimizes performance across different swarm sizes.
	
	\section{Numerical Integration}
	
	\subsection{Runge-Kutta Method (RK45)}
	
	For the numerical integration of the drone dynamics equations, we employ the Runge-Kutta method of order 5(4), commonly referred to as RK45 or the Dormand-Prince method. This method provides an adaptive step-size control mechanism that balances accuracy and computational efficiency.
	
	\subsubsection{General Runge-Kutta Formulation}
	
	A general $s$-stage Runge-Kutta method for solving the initial value problem $\dot{\mathbf{y}} = \mathbf{f}(t, \mathbf{y})$, $\mathbf{y}(t_0) = \mathbf{y}_0$ is given by:
	
	\begin{align}
		\mathbf{k}_i &= \mathbf{f}\left(t_n + c_i \Delta t, \mathbf{y}_n + \Delta t \sum_{j=1}^{s} a_{ij} \mathbf{k}_j\right), \quad i = 1, \ldots, s \\
		\mathbf{y}_{n+1} &= \mathbf{y}_n + \Delta t \sum_{i=1}^{s} b_i \mathbf{k}_i
	\end{align}
	
	where $c_i$, $a_{ij}$, and $b_i$ are method-specific coefficients typically organized in a Butcher tableau.
	
	\subsubsection{RK45 (Dormand-Prince) Coefficients}
	
	The RK45 method uses 7 stages to produce both a 5th-order accurate solution $\mathbf{y}_{n+1}$ and a 4th-order accurate solution $\hat{\mathbf{y}}_{n+1}$, which are used for error estimation and step-size control. The Butcher tableau for RK45 is:
	
	\[
	\begin{array}{c|ccccccc}
		0 & \\
		\frac{1}{5} & \frac{1}{5} & \\
		\frac{3}{10} & \frac{3}{40} & \frac{9}{40} & \\
		\frac{4}{5} & \frac{44}{45} & -\frac{56}{15} & \frac{32}{9} & \\
		\frac{8}{9} & \frac{19372}{6561} & -\frac{25360}{2187} & \frac{64448}{6561} & -\frac{212}{729} & \\
		1 & \frac{9017}{3168} & -\frac{355}{33} & \frac{46732}{5247} & \frac{49}{176} & -\frac{5103}{18656} & \\
		1 & \frac{35}{384} & 0 & \frac{500}{1113} & \frac{125}{192} & -\frac{2187}{6784} & \frac{11}{84} \\ \hline
		& \frac{35}{384} & 0 & \frac{500}{1113} & \frac{125}{192} & -\frac{2187}{6784} & \frac{11}{84} & 0 \\
		& \frac{5179}{57600} & 0 & \frac{7571}{16695} & \frac{393}{640} & -\frac{92097}{339200} & \frac{187}{2100} & \frac{1}{40}
	\end{array}
	\]
	
	The bottom row with $b_i$ coefficients gives the 5th-order solution, while the $\hat{b}_i$ coefficients (second bottom row) give the 4th-order solution used for error estimation.
	
	\subsubsection{Step-size Control}
	
	The local error estimate is computed as:
	\begin{equation}
		\mathbf{e}_{n+1} = \|\mathbf{y}_{n+1} - \hat{\mathbf{y}}_{n+1}\|
	\end{equation}
	
	Given a desired tolerance $\tau$, the optimal step size for the next step is determined by:
	\begin{equation}
		\Delta t_{\text{new}} = \Delta t_{\text{old}} \cdot \min\left(f_{\text{max}}, \max\left(f_{\text{min}}, \beta \cdot \left(\frac{\tau}{\mathbf{e}_{n+1}}\right)^{1/5}\right)\right)
	\end{equation}
	
	where $\beta \approx 0.9$ is a safety factor, and $f_{\text{min}}$, $f_{\text{max}}$ limit the step size change (typically $f_{\text{min}} = 0.1$, $f_{\text{max}} = 5.0$).
	
	\subsubsection{Implementation in \texttt{TwinkleSwarm}}
	
	In our implementation, we use SciPy's \texttt{solve\_ivp} function with the \texttt{RK45} method, specifying relative and absolute tolerances:
	\begin{equation}
		\texttt{rtol} = 5 \times 10^{-3}, \quad \texttt{atol} = 1 \times 10^{-4}
	\end{equation}
	
	These tolerances provide a good balance between accuracy and computational efficiency for our drone dynamics. The adaptive step-size control ensures that we take larger steps when the system evolves smoothly and smaller steps during rapid transients (e.g., when drones are close to each other and repulsive forces are strong).
	
	\subsubsection{Advantages for Drone Simulation}
	
	The RK45 method is particularly well-suited for drone swarm simulation because:
	\begin{itemize}
		\item \textbf{Adaptive step size}: Automatically adjusts to the local dynamics, ensuring stability during close interactions while maintaining efficiency during steady flight
		\item \textbf{High accuracy}: 5th-order local accuracy provides precise trajectory computation
		\item \textbf{Error control}: Built-in error estimation allows for controlled trade-off between accuracy and speed
		\item \textbf{Widely used}: Well-tested implementation in SciPy with excellent numerical properties
	\end{itemize}
	
	\section{Parameter Selection}
	
	Choosing appropriate parameters is crucial for desired behavior:
	
	\begin{itemize}
		\item \textbf{Mass $m$}: Can be normalized to $m=1$ for simplicity
		\item \textbf{Proportional gain $k_p$}: Higher values increase attraction strength but may cause oscillation. Typical range: $1$--$10$
		\item \textbf{Damping $k_d$}: Should satisfy $k_d \geq 2\sqrt{mk_p}$ for critical/overdamping. Typical range: $2$--$5$
		\item \textbf{Repulsion gain $k_{\text{rep}}$}: Should be strong enough to prevent collisions but not so strong as to disrupt formation. Typical range: $0.1$--$1$
		\item \textbf{Safety radius $R_{\text{safe}}$}: Should be at least twice the drone's physical radius. Typical value: $0.5$--$2$ meters
		\item \textbf{Maximum velocity $v_{\max}$}: Hardware-dependent, typically $1$--$10$ m/s
	\end{itemize}
	
	\newpage
	\begin{thebibliography}{9}
		
		\bibitem{repo}
		\texttt{TwinkleSwarm} repository: \url{https://github.com/Stochastic-Batman/TwinkleSwarm}
		
		\bibitem{lucas1981}
		B.D. Lucas and T. Kanade, "An iterative image registration technique with an application to stereo vision", \textit{Proceedings of the 7th International Joint Conference on Artificial Intelligence}, 1981.
		
		\bibitem{farneback2003}
		G. FarnebÃ¤ck, "Two-Frame Motion Estimation Based on Polynomial Expansion", \textit{Proceedings of the 13th Scandinavian Conference on Image Analysis}, 2003.
		
		\bibitem{opencv}
		OpenCV Documentation: \url{https://docs.opencv.org/4.x/}
		
		\bibitem{munkres}
		J. Munkres, "Algorithms for the assignment and transportation problems", \textit{Journal of the Society for Industrial and Applied Mathematics}, vol. 5, no. 1, pp. 32--38, 1957.
		
		\bibitem{kdtree}
		J.L. Bentley, "Multidimensional Binary Search Trees Used for Associative Searching", \textit{Communications of the ACM}, vol. 18, issue 9, pp. 509--517, 1975.
		
		\bibitem{dormand1980}
		J.R. Dormand and P.J. Prince, "A family of embedded Runge-Kutta formulae", \textit{Journal of Computational and Applied Mathematics}, vol. 6, no. 1, pp. 19--26, 1980.
		
	\end{thebibliography}
	
\end{document}